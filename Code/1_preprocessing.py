# -*- coding: utf-8 -*-
"""1_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s365YmWBg6FGxgZQRBUvVpcCsPMOh1LD

##  1_Preprocessing.ipynb – Dataset Cleaning & Sampling

This notebook prepares the Lending Club dataset for modeling.  
It filters relevant rows and columns, creates the combined BERT-compatible text field, and generates a balanced sample for efficient training.

**Main outputs:**
- `loan_data_cleaned.csv`: Cleaned full dataset (2015–2016 loans)
- `loan_data_sampled.csv`: Balanced subset (10,000 fully paid + 10,000 charged off)

### Step 1: Load the Lending Club Dataset

First, we mount Google Drive and load the full Lending Club dataset from the provided CSV.  
Since it’s a large file (~2.2M rows and 151 columns), we’ll take a quick look at its shape and structure before moving ahead.
"""

from google.colab import drive
drive.mount('/content/drive')

data_path = '/content/drive/MyDrive/LoanApprovalProject/Data/accepted_2007_to_2018Q4.csv'

import pandas as pd

df_raw = pd.read_csv(data_path, low_memory=False)

print("Dataset shape:", df_raw.shape)
print("\nColumn count:", len(df_raw.columns))

df_raw.head()

"""### Step 2: Filter and Select Relevant Columns

We don’t need all rows or columns.  
Here, we keep only loans from 2015 and 2016 and select just the relevant features (numerical, categorical, and text) for our models.  
We also convert the target column `loan_status` into binary:  
`Fully Paid → 1`, `Charged Off → 0`.
"""

df = df_raw[df_raw['issue_d'].astype(str).str.contains('2015|2016', na=False)]

# Columns I want to keep for modeling
selected_columns = [
    'loan_amnt', 'term', 'int_rate', 'grade', 'emp_title', 'home_ownership',
    'annual_inc', 'verification_status', 'purpose', 'title', 'dti',
    'open_acc', 'revol_util', 'fico_range_high', 'loan_status'
]

df = df[selected_columns].copy()

# Drop rows missing the target or key numerical values
df = df.dropna(subset=['loan_status', 'annual_inc'])

# Convert loan_status to binary: Fully Paid → 1, Charged Off → 0
df = df[df['loan_status'].isin(['Fully Paid', 'Charged Off'])]
df['loan_status'] = df['loan_status'].map({'Fully Paid': 1, 'Charged Off': 0})

print("Shape after filtering and column selection:", df.shape)
df.head()

"""# Step 3: Create the Combined Text Field for BERT

Since I’ll be using BERT for the text input, I’m creating a new column that combines the `purpose` and `title` fields into one. I’ll fill missing titles with “Unknown” to avoid issues later.

"""

df['title'] = df['title'].fillna('Unknown')

# Combine purpose and title into a new text column
df['text'] = df['purpose'].astype(str) + ' - ' + df['title'].astype(str)

# Drop the original text source columns
df.drop(['purpose', 'title'], axis=1, inplace=True)

print("Shape after creating text column:", df.shape)
df[['text']].head()

"""### Step 4: Save Cleaned Dataset

Now that we’ve filtered and cleaned the dataset, we save the result to a new file:  
`loan_data_cleaned.csv` — this will be used in the modeling steps.
"""

output_path = '/content/drive/MyDrive/LoanApprovalProject/Data/loan_data_cleaned.csv'

df.to_csv(output_path, index=False)

print(" Saved cleaned dataset to:", output_path)

"""### Step 5: Create a Balanced Sample (~20,000 rows)

To keep training efficient and balanced, we randomly sample 10,000 examples from each class.  
We save this as `loan_data_sampled.csv` for the upcoming training steps.
"""

df_sampled = (
    df.groupby('loan_status')
      .apply(lambda x: x.sample(n=10000, random_state=42))
      .reset_index(drop=True)
)

sampled_path = '/content/drive/MyDrive/LoanApprovalProject/Data/loan_data_sampled.csv'
df_sampled.to_csv(sampled_path, index=False)

print(" Saved sampled dataset:", df_sampled.shape)
df_sampled['loan_status'].value_counts()